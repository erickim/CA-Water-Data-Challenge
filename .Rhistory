install.packages("googlesheets")
library(googlesheets)
gs_auth(new_user = TRUE)
gs_ls()
gs_key(key)
key <- "1NhC1WyYVj2K4N-JpoYQk_7W0ZAbDInjmiYirHUkBaOE"
gs_key(key)
library(readxl)
datasets_gsheets <- read_excel("data/datasets-gsheets.xlsx")
library(readxl)
datasets_gsheets <- read_excel("data/datasets-gsheets.xlsx")
excel_sheets("data/datasets-gsheets.xlsx")
sheets <- excel_sheets("data/datasets-gsheets.xlsx")
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
# gsheets doc with the datasets was downloaded 9/6/18 in the am
library(readxl)
gsheet_path <- "data/datasets-gsheets.xlsx"
sheets <- excel_sheets(gsheet_path)
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
# gsheets doc with the datasets was downloaded 9/6/18 in the am
library(readxl)
gsheet_path <- "data/datasets-gsheets.xlsx"
sheets <- excel_sheets(gsheet_path)
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
names(docs)
names(docs) <- sheets
docs
docs$sheets[1]
docs$eval(sheets[1])
# gsheets doc with the datasets was downloaded 9/6/18 in the am
library(readxl)
gsheet_path <- "data/datasets-gsheets.xlsx"
sheets <- excel_sheets(gsheet_path)
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
docs
for (i in sheets) {
system(paste("mkdir data/", i))
}
paste("mkdir data/", i)
for (i in sheets) {
system(paste0("mkdir data/", i))
}
for (i in sheets) {
paste0("mkdir data/", i)
}
for (i in sheets) {
print(paste0("mkdir data/", i))
}
docs
docs[[1]]
docs[[1]]$`Link to Open Data Format`
library(tidyverse)
docs %>% map(1)
docs %>% map("name")
docs %>% map("names")
docs
## !! you only need to run this once !!
# gsheets doc with the datasets was downloaded 9/6/18 at 7 am
# there may have been more links added since
library(readxl)
library(tidyverse)
gsheet_path <- "data/datasets-gsheets.xlsx"
sheets <- excel_sheets(gsheet_path)
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
names(docs) <- sheets
docs %>% map("names")
docs %>% map("name")
docs %>% pull("Recommended")
docs %>% extract("Recommended")
docs %>% pluck("Recommended")
docs %>% pluck("Recommended") %>% pull(`Link to Open Data Format`)
docs %>% pluck("Recommended") %>% filter(grep("http", `Link to Open Data Format`))
?grep
docs %>% pluck("Recommended") %>% filter(grepl("http", `Link to Open Data Format`))
docs %>% pluck("Recommended") %>% filter(grepl("http", `Link to Open Data Format`)) %>% nrow
nrow(docs$Recommended)
docs[[2]]
links <- lapply(sheets[1:2],
function(sheet) {
sheet <- enquo(sheet)
docs %>%
pluck(!!sheet) %>%
filter(grepl("http", `Link to Open Data Format`)) %>%
pull(`Link to Open Data Format`)
})
links <- lapply(sheets[1:2],
function(sheet) {
sheet <- enquo(sheet)
docs %>%
pluck(sheet) %>%
filter(grepl("http", `Link to Open Data Format`)) %>%
pull(`Link to Open Data Format`)
})
links <- lapply(sheets[1:2],
function(sheet) {
#sheet <- enquo(sheet)
docs %>%
pluck(sheet) %>%
filter(grepl("http", `Link to Open Data Format`)) %>%
pull(`Link to Open Data Format`)
})
links[[1]]
links[[2]]
links[[1]][1]
library(rvest)
read_html(links[[1]][1])
x <- read_html(links[[1]][1])
x$node
x$doc
?read_html
x <- read_xml(links[[1]][1])
read_html("http://had.co.nz")
read_html("http://had.co.nz")$node
x <- read_html(links[[1]][1]) %>%
html_nodes(xpath = "//input[contains(@name,'_ctl0:PlaceHolderContenido')]") %>%
html_attrs()
x
?rvest
??rvest
read_html(links[[1]][1]) %>%
html_nodes("link") %>%
html_text()
read_html(links[[1]][1]) %>%
html_nodes("link")
read_html(links[[1]][1]) %>%
html_nodes("a")
read_html(links[[1]][1]) %>%
html_nodes("a") %>% html_attr("href")
read_html(links[[1]][1]) %>%
html_nodes("a") %>% html_attr("href") %>% grepl("pdf")
read_html(links[[1]][1]) %>%
html_nodes("a") %>% html_attr("href") %>% grepl(pattern = "pdf")
read_html(links[[1]][1]) %>%
html_nodes("a") %>% html_attr("href") %>% grepl(pattern = "csv")
?grepl
read_html(links[[1]][1]) %>%
html_nodes("a") %>%
html_attr("href") %>%
grepl(pattern = "pdf|csv")
tidyverse_packages()
?str_Extract
?str_rxtract
?str_extract
read_html(links[[1]][1]) %>%
html_nodes("a") %>%
html_attr("href") %>%
str_extract(pattern = "pdf|csv")
read_html(links[[1]][1]) %>%
html_nodes("a") %>%
html_attr("href") %>%
str_extract_all(pattern = "pdf|csv")
?grepl
read_html(links[[1]][1]) %>%
html_nodes("a") %>%
html_attr("href") %>%
gregexpr(pattern = "pdf|csv")
read_html(links[[1]][1]) %>%
html_nodes("a") %>%
html_attr("href") %>%
grepl(pattern = "pdf|csv", fixed = T)
?`[`
`[`
read_html(links[[1]][1]) %>%
html_nodes("a") %>%
html_attr("href") %>%
data.frame(links = .) %>%
filter(grepl(pattern = "pdf|csv", links))
read_html(links[[1]][1]) %>%
html_nodes("a") %>%
html_attr("href") %>%
data.frame(links = .) %>%
filter(grepl(pattern = "pdf|csv", links)) %>%
pull(links)
names(links)
links[[1]]
links[[2]]
download_data <- function(links) {
links %>%
read_html() %>%
html_nodes("a") %>%
html_attr("href") %>%
data.frame(links = .) %>%
filter(grepl(pattern = "pdf|csv", links)) %>%
pull(links)
}
links[[1]] %>% download_data
links[[1]] %>% lapply(download_data)
links[[1]][3]
links[[1]][16]
download_data <- function(links) {
links %>%
read_html() %>%
html_nodes("a") %>%
html_attr("href") %>%
data.frame(links = .) %>%
filter(grepl(pattern = "pdf|csv|json", links)) %>%
pull(links)
}
links[[1]][16] %>% download_data
docs %>% pluck(sheets[1])
## !! you only need to run this once !!
# gsheets doc with the datasets was downloaded 9/6/18 at 7 am
# there may have been more links added since
library(readxl)
library(tidyverse)
library(rvest)
gsheet_path <- "data/datasets-gsheets.xlsx"
sheets <- excel_sheets(gsheet_path)
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
names(docs) <- sheets
# if youre on pc this probably wont work
for (i in sheets) {
system(paste0("mkdir data/", i))
}
links <- lapply(sheets[1:2],
function(sheet) {
docs %>%
pluck(sheet) %>%
filter(grepl("http", `Link to Open Data Format`)) %>%
pull(`Link to Open Data Format`)
})
names(links) <- sheets
names(links) <- sheets[1:2]
sheets
gsub(sheets, " ", "\ ")
?gsub
gsub(" ", "\ ", sheets)
gsub(" ", "\\ ", sheets)
gsub(" ", "\\\ ", sheets)
gsub("\ ", "\\ ", sheets)
gsub("\ ", "\", sheets)
gsub(" ", "\\", sheets)
gsub(" ", "-", sheets)
## !! you only need to run this once !!
# gsheets doc with the datasets was downloaded 9/6/18 at 7 am
# there may have been more links added since
library(readxl)
library(tidyverse)
library(rvest)
gsheet_path <- "data/datasets-gsheets.xlsx"
sheets <- gsub(" ", "-", excel_sheets(gsheet_path))
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
names(docs) <- sheets
# if youre on pc this probably wont work
for (i in sheets) {
system(paste0("mkdir data/", i))
}
## !! you only need to run this once !!
# gsheets doc with the datasets was downloaded 9/6/18 at 7 am
# there may have been more links added since
library(readxl)
library(tidyverse)
library(rvest)
gsheet_path <- "data/datasets-gsheets.xlsx"
sheets <- excel_sheets(gsheet_path)
sheets_dir <- gsub(" ", "-", sheets)
docs <- lapply(sheets,
function(sheet) {
read_excel(gsheet_path, sheet = sheet)
})
names(docs) <- sheets
# if youre on pc this probably wont work
for (i in sheets_dir) {
system(paste0("mkdir data/", i))
}
links <- lapply(sheets[1:2],
function(sheet) {
docs %>%
pluck(sheet) %>%
filter(grepl("http", `Link to Open Data Format`)) %>%
pull(`Link to Open Data Format`)
})
docs %>% pluck(sheets[1])
doc
docs
sheets
docs %>% pluck(sheets[1])
?plucks
?pluck
detach("package:rvest", unload=TRUE)
links <- lapply(sheets[1:2],
function(sheet) {
docs %>%
pluck(sheet) %>%
filter(grepl("http", `Link to Open Data Format`)) %>%
pull(`Link to Open Data Format`)
})
# gsheets doc with the datasets was downloaded 9/6/18 at 7 am
# there may have been more links added since
library(rvest)
tidyverse_conflicts()
library(tidyverse)
detach("package:purrr", unload=TRUE)
library("purrr", lib.loc="/Library/Frameworks/R.framework/Versions/3.5/Resources/library")
# I'm looking to use purrr's pluck here, not rvest's
links <- lapply(sheets[1:2],
function(sheet) {
docs %>%
pluck(sheet) %>%
filter(grepl("http", `Link to Open Data Format`)) %>%
pull(`Link to Open Data Format`)
})
names(links) <- sheets[1:2]
download_data <- function(links) {
links %>%
read_html() %>%
html_nodes("a") %>%
html_attr("href") %>%
data.frame(links = .) %>%
filter(grepl(pattern = "pdf|csv|json", links)) %>%
pull(links)
}
links[[1]][16] %>% download_data
links[[1]]
?download.file
links %>% pluck(sheets[1])
links %>% pluck(sheets[1]) %>% pluck(1)
# ex:
links %>% pluck(sheets[1]) %>% pluck(1) %>% download_data
system("rm .httr-oauth")
